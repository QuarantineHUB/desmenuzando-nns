{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Desmenuzando a las neuronas artificiales (Resuelto)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R6jO_1gISKxk",
        "colab_type": "text"
      },
      "source": [
        "# **Tutorial:** Desmenuzando a las neuronas artificiales\n",
        "\n",
        "> Rodolfo Ferro <br>\n",
        "> <https://github.com/RodolfoFerro> <br>\n",
        "> Future Lab, 2020."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uehq48zoSocy",
        "colab_type": "text"
      },
      "source": [
        "Nos basaremos con la primer abstracción de una neurona artificial, llamada **perceptrón** y desarrollada por el científico **Frank Rosenblatt**.\n",
        "\n",
        "### **Entonces, ¿qué es un perceptrón?**\n",
        "\n",
        "Un perceptrón es una abstracción de una neurona real.\n",
        "\n",
        "Éste toma varias **entradas** $x_1, x_2,..., x_n $ y produce una **salida**. Para la salida, Rosenblatt propuso que las entradas tuviesen **pesos** asciados $w_1, w_2, ..., w_n$, siendo estos números reales que expresan la importancia respectiva de cada entrada para la salida. La salida de la neurona, $0$ o $1$, está determinada con base en que la suma ponderada, \n",
        "\n",
        "$$\\displaystyle\\sum_{j}w_jx_j,$$\n",
        "\n",
        "<!-- $\\textbf{w}_{Layer}\\cdot\\textbf{x} = \n",
        "\\begin{bmatrix}\n",
        "w_{1, 1} & w_{1, 2} & \\cdots & w_{1, n}\\\\\n",
        "w_{2, 1} & w_{2, 2} & \\cdots & w_{2, n}\\\\\n",
        "\\vdots & \\vdots & \\ddots & \\vdots\\\\\n",
        "w_{m, 1} & w_{m, 2} & \\cdots & w_{m, n}\\\\\n",
        "\\end{bmatrix} \\cdot\n",
        "\\begin{bmatrix}\n",
        "x_1\\\\\n",
        "x_2\\\\\n",
        "\\vdots\\\\\n",
        "x_n\n",
        "\\end{bmatrix}$ -->\n",
        "\n",
        "(para $j \\in \\{1, 2, ..., n\\}$ ) sea menor o mayor que un **valor límite** que por ahora llamaremos umbral.\n",
        "\n",
        "Resumiendo, un perceptron es un sistema que toma decisiones con base en la evidencia presentada."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q33kCpXyFgJ_",
        "colab_type": "text"
      },
      "source": [
        "#### **Construyamos un ejemplo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLBMuek3lBHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "\n",
        "# Primero creamos nuestra clase perceptron\n",
        "class Perceptron():\n",
        "    def __init__(self, inputs, weights):\n",
        "        self.inputs = np.array(inputs)\n",
        "        self.weights = np.array(weights)\n",
        "        self.size = len(inputs)\n",
        "  \n",
        "    def decide(self, treshold):\n",
        "        if (self.weights @ self.inputs) <= treshold:\n",
        "            print(\"Híjoles, no se va a poder...\")\n",
        "        else:\n",
        "            print(\"Puedes ir.\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t42O74IdmKIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Ahora necesitamos darle sus entradas y pesos asociados\n",
        "inputs, weights = [], []\n",
        "\n",
        "preguntas = [\n",
        "    \"· ¿Es un tema que me interese? \",\n",
        "    \"· ¿Tengo botana? \",\n",
        "    \"· ¿Mi conexión a internet es estable? \"\n",
        "]\n",
        "\n",
        "for pregunta in preguntas:\n",
        "    i = int(input(pregunta))\n",
        "    w = int(input(\"· Y su peso asociado es... \"))\n",
        "    inputs.append(i)\n",
        "    weights.append(w)\n",
        "    print()\n",
        "\n",
        "treshold = int(input(\"· Y nuestro umbral/límite será: \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHjy-k33oNFm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "p = Perceptron(inputs, weights)\n",
        "p.decide(treshold)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUCCwUG6DgCX",
        "colab_type": "text"
      },
      "source": [
        "### **Bias y funciones de activación**\n",
        "\n",
        "_Antes de seguir, introduciremos otro concepto, que es el **bias**._\n",
        "\n",
        "La operación matemática que realiza la neurona se puede escribir como:\n",
        "\n",
        "$$ f(\\textbf{x}) = \n",
        "  \\begin{cases}\n",
        "    0 & \\text{si $\\displaystyle\\sum_{j}w_jx_j <$ valor límite o treshold} \\\\\n",
        "    1 & \\text{si $\\displaystyle\\sum_{j}w_jx_j \\geq$ valor límite o treshold} \\\\\n",
        "  \\end{cases},$$\n",
        "\n",
        "donde $\\textbf{x} = (x_1, x_2, ..., x_n)$ y $j \\in \\{1, 2, ..., n\\}$.\n",
        "\n",
        "De lo anterior, podemos despejar el valor límite (el umbral) y escribirlo como $b$, obteniendo:\n",
        "\n",
        "$$ f(\\textbf{x}) = \n",
        "  \\begin{cases}\n",
        "    0 & \\text{si $\\displaystyle\\sum_{j}w_jx_j + b < 0$} \\\\\n",
        "    1 & \\text{si $\\displaystyle\\sum_{j}w_jx_j + b > 0$} \\\\\n",
        "  \\end{cases},$$\n",
        "\n",
        "donde $\\textbf{x} = (x_1, x_2, ..., x_n)$ y $j \\in \\{1, 2, ..., n\\}$.\n",
        "\n",
        "Esto que escribimos como $b$, también se le conoce como **bias**, y describe *qué tan susceptible la red es a __dispararse__*.\n",
        "\n",
        "Curiosamente, esta descripción matemática encaja con la función de salto, que es una función de activación. Esto es, una función que permite el paso de información de acuerdo a la entrada y los pesos, permitiendo el disparo del lo procesado hacia la salida. La función de salto se ve como sigue:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/4/4a/Funci%C3%B3n_Cu_H.svg\" width=\"40%\" alt=\"Función escalón de Heaviside\">\n",
        "</center>\n",
        "\n",
        "Sin embargo, podemos hacer a una neurona aún más susceptible con respecto a los datos de la misma (entradas, pesos, bias) añadiendo una función sigmoide. La función sigmoide se ve como a continuación: \n",
        "\n",
        "<center>\n",
        "    <img src=\"https://upload.wikimedia.org/wikipedia/commons/6/66/Funci%C3%B3n_sigmoide_01.svg\" width=\"40%\" alt=\"Función sigmoide\">\n",
        "</center>\n",
        "\n",
        "Esta función es suave, y por lo tanto tiene una diferente \"sensibililad\" a los cambios abruptos de valores. También, sus entradas en lugar de solo ser $1$'s o $0$'s, pueden ser valores en todos los números reales. La función sigmoide es descrita por la siguiente expresión matemática:\n",
        "\n",
        "$$f(z) = \\dfrac{1}{1+e^{-z}}$$\n",
        "\n",
        "O escrito en términos de pesos y biases:\n",
        "\n",
        "$$f(z) = \\dfrac{1}{1+\\exp{\\left\\{-\\left(\\displaystyle\\sum_{j}w_jx_j +b\\right)\\right\\}}}$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0G1MY4HQFsEd"
      },
      "source": [
        "#### **Volviendo al ejemplo**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSn8VaEoDtHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Modificamos para añadir la función de activación\n",
        "class SigmoidNeuron():\n",
        "    def __init__(self, inputs, weights):\n",
        "        self.inputs = np.array(inputs)\n",
        "        self.weights = np.array(weights)\n",
        "  \n",
        "    def decide(self, bias):\n",
        "        z = (self.weights @ self.inputs) + bias\n",
        "        return 1. / (1. + np.exp(-z))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ogPy6NpfERfJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "bias = int(input(\"· El nuevo bias será: \"))\n",
        "s = SigmoidNeuron(inputs, weights)\n",
        "s.decide(bias)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRGlbVZsFxdk",
        "colab_type": "text"
      },
      "source": [
        "> Esta es la neurona que usaremos para los siguientes tópicos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvmIk2G9EgOQ",
        "colab_type": "text"
      },
      "source": [
        "<center>\n",
        "    *********\n",
        "</center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Km7Ow80drf2",
        "colab_type": "text"
      },
      "source": [
        "### **Redes de neuronas artificiales**\n",
        "\n",
        "Imaginemos que en lugar de una única neurona, tenemos 7 neuronas, todas con diferentes pesos (que inicialmente definiremos de manera aleatoria), dispuestas con la siguiente configuración: \n",
        "- Tenemos dos neuronas que representarán la información de entrada\n",
        "- Cada una de las dos neuronas está conectada con cuatro siguientes neuronas dispuestas en una siguiente capa\n",
        "- Finalmente, cada salida de las cuatro neuronas previas es conectada a una única neurona dispuesta en una capa de salida\n",
        "\n",
        "La configuración quedaría como se muestra a continuación:\n",
        "\n",
        "<center>\n",
        "    <img src=\"https://www.pngitem.com/pimgs/m/531-5314899_artificial-neural-network-png-transparent-png.png\" width=\"30%\" alt=\"A simple neural network\">\n",
        "</center>\n",
        "\n",
        "La previa configuración de neuronas dispuesta en forma de red podría tomar decisiones más complejas y abstractas, si consideramos la operación definida anteriormente (pesos, bias, función de activación) sobre los pesos de cada conexión de neuronas entre las capas.\n",
        "\n",
        "Esto, a grandes rasgos, es una manera de abstraer una red neuronal artifical, compuesta por neuronas como previamente hemos definido; y donde se tienen capas de neuronas cuyas salidas funcionan como las entradas de otras neuronas (en las siguientes capas).\n",
        "\n",
        "Con esto en mente, podemos programar una red neuronal artificial con la configuración predefinida."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IB5BVMaXNmCF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from numpy.random import uniform as u\n",
        "\n",
        "\n",
        "class NeuralNet:\n",
        "    \"\"\"Objeto NeuralNet para crear redes neuronales artificiales.\n",
        "\n",
        "\n",
        "    Por simplicidad, supondremos que la capa de entrada siempre será de\n",
        "    dos neuronas y la capa de salida siempre de una, con el fin de poder\n",
        "    visualizar el funcionamiento interno de la red como un mapeo\n",
        "    $\\mathbb{R}^2 \\to \\mathbb{R}$.\"\"\"\n",
        "\n",
        "    def __init__(self, n_layers=1, n_neurons=4):\n",
        "        \"\"\"Constructor de la clase.\n",
        "        \n",
        "        Inicializa pesos aleatorios en la red neuronal.\n",
        "        \n",
        "        Parámetros\n",
        "        ----------\n",
        "        n_capas : int\n",
        "            Número de capas ocultas en la red. Por default, el valor\n",
        "            es 1.\n",
        "        n_neuronas : int\n",
        "            Número de neuronas por capa en la red. Por default, el\n",
        "            valor es 4.\n",
        "        \"\"\"\n",
        "\n",
        "        # Definimos pesos y biases en la capa de entrada\n",
        "        self.w_in = u(low=-1, high=1, size=(2, n_neurons))\n",
        "        self.b_in = u(low=-1, high=1, size=n_neurons)\n",
        "\n",
        "        # Definimos pesos y biases en las capas ocultas\n",
        "        self.w_hidden = u(low=-3, high=3, size=(n_layers, n_neurons, n_neurons))\n",
        "        self.b_hidden = u(low=-1, high=1, size=(n_layers, n_neurons))\n",
        "\n",
        "        # Definimos pesos y biases en las capa de salida\n",
        "        self.w_out = u(low=-1, high=1, size=(n_neurons, 1))\n",
        "        self.b_out = u(low=-1, high=1, size=1)\n",
        "\n",
        "    @staticmethod\n",
        "    def activate_layer(y_in, w, b):\n",
        "        \"\"\"Calcula el producto interno y aplica función de activación\n",
        "        por capa.\n",
        "        \n",
        "        Parámetros\n",
        "        ----------\n",
        "        y_in : ndarray\n",
        "            El vector con datos de entrada.\n",
        "        w : ndarray\n",
        "            El vector de pesos.\n",
        "        b : ndarray\n",
        "            El vector de biases.\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        s : ndarray\n",
        "            Un vector de activación reultante.\n",
        "        \"\"\"\n",
        "\n",
        "        z = np.dot(y_in, w) + b\n",
        "        s = 1. / (1. + np.exp(-z))\n",
        "        \n",
        "        return s\n",
        "\n",
        "    def feedforward(self, y_in):\n",
        "        \"\"\"Calcula el producto interno y aplica función de activación\n",
        "        por capa.\n",
        "        \n",
        "        Parámetros\n",
        "        ----------\n",
        "        y_in : ndarray\n",
        "            El vector con datos de entrada.\n",
        "        w : ndarray\n",
        "            El vector de pesos.\n",
        "        b : ndarray\n",
        "            El vector de biases.\n",
        "        \n",
        "        Retorna\n",
        "        -------\n",
        "        s : ndarray\n",
        "            Un vector de activación reultante.\"\"\"\n",
        "\n",
        "        y = self.activate_layer(y_in, self.w_in, self.b_in)\n",
        "        for i in range(self.w_hidden.shape[0]):  # Número de capas ocultas\n",
        "            y = self.activate_layer(y, self.w_hidden[i], self.b_hidden[i])\n",
        "        output = self.activate_layer(y, self.w_out, self.b_out)\n",
        "        \n",
        "        return output\n",
        "    \n",
        "    def visualize(self, grid_size=50, colormap='viridis', c_reverse=False):\n",
        "        \"\"\"Función para visualizar el mapeo de la red neuronal en un \n",
        "        plano 2D.\n",
        "        \n",
        "        Parámetros\n",
        "        ----------\n",
        "        grid_size : int\n",
        "            El tamaño a utlizar para crear rejilla. La rejilla se crea de \n",
        "            tamaño (grid_size, grid_size). El valor default es 50.\n",
        "        colormap : str\n",
        "            El mapa de color a utilizar. El valor default es 'viridis'.\n",
        "        c_reverse : bool\n",
        "            Flag para especificar si se invierte el mapa de color. El valor\n",
        "            default es False.\n",
        "        \"\"\"\n",
        "\n",
        "        import matplotlib.pyplot as plt\n",
        "        import matplotlib as mpl\n",
        "        mpl.rcParams['figure.dpi'] = 300\n",
        "\n",
        "        # Creamos una rejilla\n",
        "        x = np.linspace(-0.5, 0.5, grid_size)\n",
        "        y = np.linspace(-0.5, 0.5, grid_size)\n",
        "        xx, yy = np.meshgrid(x, y)\n",
        "\n",
        "        # Para todas las coordenadas (x, y) en la rejilla,\n",
        "        # hacemos una única lista con los pares de puntos\n",
        "        x_flat = xx.flatten()\n",
        "        y_flat = yy.flatten()\n",
        "        y_in = zip(x_flat, y_flat)\n",
        "        y_in = np.array(list(y_in))\n",
        "\n",
        "        # Hacemos feedforward con la red\n",
        "        y_out = self.feedforward(y_in)\n",
        "        \n",
        "        # Redimensionamos a la rejilla\n",
        "        y_out_2d = np.reshape(y_out, (grid_size, grid_size))\n",
        "\n",
        "        if c_reverse:\n",
        "            cmap = plt.cm.get_cmap(colormap)\n",
        "            cmap = cmap.reversed()\n",
        "        else:\n",
        "            cmap = colormap\n",
        "        \n",
        "        # Graficamos los resultados de la red\n",
        "        plt.figure(figsize=(10, 10))\n",
        "        plt.axes([0, 0, 1, 1])\n",
        "        plt.imshow(\n",
        "            y_out_2d,\n",
        "            extent=[-0.5, 0.5, -0.5, 0.5],\n",
        "            interpolation='nearest',\n",
        "            cmap=cmap\n",
        "        )\n",
        "        plt.axis(False)\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NbBdOCcBxIu",
        "colab_type": "text"
      },
      "source": [
        "Ahora creamos un vector de entrada con valores aleatorios, que utilizaremos para validar el funcionamiento de nuestra red neuronal."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qn7CmHvaBxIw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Definimos los valores de entrada\n",
        "y_in = np.array([0.8, 0.2])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8jqhWL2blry",
        "colab_type": "text"
      },
      "source": [
        "Creamos una instancia de nuestra clase, una red neuronal artificial con la configuración previamente definida (1 capa oculta de 4 neuronas)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NGpbPu6zSSFO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn = NeuralNet()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0FYerYYbw_x",
        "colab_type": "text"
      },
      "source": [
        "Realizamos el proceso _feedforward_ de la red con el valor de entrada."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQ5PeIVCb0g_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.feedforward(y_in)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlcpcBR-BxI6",
        "colab_type": "text"
      },
      "source": [
        "#### **¿Y si profundizamos en el tema?**\n",
        "\n",
        "Aplicamos el producto punto de matrices y agregamos el bias y evaluamos en la función sigmoide."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOsvH_zCK0Gq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deep_nn.visualize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BubDmxPK7gT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deep_nn = NeuralNet(n_layers=20, n_neurons=100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OCMY2BbrMoLP",
        "colab_type": "text"
      },
      "source": [
        "#### **Visualización**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "libUPp9ccics",
        "colab_type": "text"
      },
      "source": [
        "Hagamos un pequeño análisis de lo que sucede al visualizar la salida de nuestra red neuronal con una configuración pequeña."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q4xQ04VYYd3X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nn.visualize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7czmGUWScwOP",
        "colab_type": "text"
      },
      "source": [
        "En cambio, ¿qué sucede con nuestra red neuronal profunda?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chR_4buaYhGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deep_nn.visualize()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wk1j327Qc65K",
        "colab_type": "text"
      },
      "source": [
        "¿Esta imagen valida relaciones más profnudas?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCnQ2W1qdBXQ",
        "colab_type": "text"
      },
      "source": [
        "### **Retos**\n",
        "\n",
        "1. Visualización en HD.\n",
        "2. Implementar otras funciones de activación.\n",
        "3. Visualizar capas intermedias.\n",
        "4. Explorar con variaciones de valores en los pesos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OexocUNId1hG",
        "colab_type": "text"
      },
      "source": [
        "## **Comentarios extra**\n",
        "\n",
        "- Sobre el aprendizaje\n",
        "- Sobre aplicaciones reales\n",
        "- ¿Algo más?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rKsqcQn1fUDV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}